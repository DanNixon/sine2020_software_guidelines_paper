% add. options: [seceqn,secthm,crcready,onecolumn]
\documentclass[jnr]{iosart2x}

%\usepackage{dcolumn}
%\usepackage{endnotes}

%%%%%%%%%%% Put your definitions here

\usepackage{listings}

%%%%%%%%%%% End of definitions

\pubyear{0000}
\volume{0}
\firstpage{1}
\lastpage{1}

\begin{document}

\begin{frontmatter}

%\pretitle{}
\title{Towards development of sustainable data treatment software}
\runningtitle{Towards development of sustainable data treatment software}
%\subtitle{}

% Two or more authors:
\author[A]{\inits{D.}\fnms{Daniel} \snm{Nixon}\ead[label=e1]{daniel.nixon@stfc.ac.uk}%
\thanks{Corresponding author. \printead{e1}.}},
\author[A]{\inits{A.}\fnms{Anthony} \snm{Lim}\ead[label=e2]{anthony.lim@stfc.ac.uk}},
\author[A]{\inits{A.}\fnms{Anders} \snm{Markvardsen}\ead[label=e3]{anders.markvardsen@stfc.ac.uk}},
\author[B]{\inits{T.}\fnms{Thomas} \snm{Holm Rod}\ead[label=e4]{Thomas.HolmRod@esss.se}},
\author[C]{\inits{A.}\fnms{Anders} \snm{Kaestner}\ead[label=e5]{anders.kaestner@psi.ch}},
\author[D]{\inits{J.}\fnms{Joachim} \snm{Wuttke}\ead[label=e6]{j.wuttke@fz-juelich.de}},
\author[A]{\inits{S.}\fnms{Stephen} \snm{Cottrell}\ead[label=e7]{stephen.cottrell@stfc.ac.uk}},
and
\author[E]{\inits{M.}\fnms{Miguel} \snm{Gonzalez}\ead[label=e8]{gonzalezm@ill.fr}}

\runningauthor{D. Nixon et al.}
\address[A]{\orgname{Science and Technology Facilities Council},\cny{United Kingdom}\printead[presep={\\}]{e1,e2,e3,e7}}
\address[B]{\orgname{European Spallation Source},\cny{Sweden}\printead[presep={\\}]{e4}}
\address[C]{\orgname{Paul Scherrer Institute},\cny{Switzerland}\printead[presep={\\}]{e5}}
\address[D]{\orgname{Forschungszentrum Jülich},\cny{Germany}\printead[presep={\\}]{e6}}
\address[E]{\orgname{Institut Laue-Langevin},\cny{France}\printead[presep={\\}]{e8}}

\begin{abstract}
TODO:
- How to make software outlive the people developing it
- SINE2020
- Preventing loss of work
\end{abstract}

\begin{keyword}
\kwd{TODO}
\end{keyword}

\end{frontmatter}

%%%%%%%%%%% The article body starts:

\section{Introduction}
\label{Introduction}

The advances in science are leading to vast amounts of data that cannot be analysed by hand. For example, the European Spallation Source (ESS) will have data rates of (TODO: numbers and ref). 
The importance of simulations and complex analysis techniques has increased inorder to determine the underlying science. 
The consequence of this is that the software is often complex and requires a significant investment to develop. 
Therefore, it is benefical to develop code that is easy to maintain, extend (new functionality) and does not have duplicated effort.  
This can be achieved by a using guidelines and all maintable high performing software will have at least some guidelines.

The Oxford Dictionaries (https://en.oxforddictionaries.com/) definition of a guideline is "a general rule, principle, or piece of advice", for example “the organisation has issued guidelines for people working with prisoners”.
We will use this definition throughout the remainder of this paper.

Traditionally within the research community people would create and develop their own analysis software packages.
These could be in the form of scripts (e.g. Python) or compiled code (e.g. Fortran, C++).
This software is often unique to the research group and effectively leads to duplicated effort as other research groups develop software for the same purpose.
The developers of the software frequently change (due to the nature of post docs) and without guidelines this can lead to the code being difficult to read because the style will change depending on who wrote it.
This in the worst cases can lead to people believing that the code is incorrect, when it is correct, hence leading to wasted effort.
The other potential pitfalls with this isolated approach of software development is that bugs are less likely to be found, due to less users.
Also the software may become too cumbersome to develop leading to the project having to start from scratch.
Inherently, these factors make it more difficult to share and for other research groups or research infrastructures to adapt the software for their own purpose.

Guidelines for software development ensure that the code is of a high quality and that the project progress effectively. A good set of guidlines will help the software be stable and extensible. Therefore, it will be easier to continue growing the code base to include new scientiifc analysis/techniques. Some areas have specific needs and requirments that may lead to additional/different guidlines \cite{} (TODO: add papers) that are tailored to their specific user cases. 

Guidelines have been used for sceintific software development as part of SINE2020. These include small projects such as BornAgain \cite{}, McStats \cite{} and MuhRec \cite{}. 
Th Mantid \cite{} and SasView \cite{} (TODO: add citations) codes have large teams (up to about 20 people) and need guidelines to ensure a consistant quality across the code base. 
In the case of Mantid and SasView the need for guidelines is further increased by having a team that is distributed across the world. 
For more information on how each of these projects uses guidelines please see \cite{} (TODO: ref report).   

The guidelines presented in this paper will suggest the best practises for the current tools that are commonly available today.
Hence, when it is appropriate the idea of the guideline will be abstracted so that an alternative can be chosen in the future. 

\section{Business as Usual}
\label{Business as Usual}

This section describes the day to day work of the developers working on the software.
Most of these choices will be made at the start of the project and ideally last its duration, however these can be updated to accommodate new tools and standards, etc.

This section will discuss the typical tasks performed by developers, such as development methodologies, code review and coding standards. Another important part of the developers job is to provide documentation for users and other developers. This reduces the risk of having single points of failure, because another developer can read the documentation and then continue the work. 

\subsection{Development methodology}
\label{Development methodology}

Development methodology is the set of rules that determine how requirements are gathered, converted into developer tasks and how the software is delivered to the user.

The two extremes of software development methodologies are called "agile" and "waterfall". In the following we will discuss these to extremes, however in reality there is a scale and a mixture of these two extremes can be used. 

For scientific software agile, or some variant of that, is the most comman methodology. The general principle of agile are short development "sprints" in which small, incremental additions are made to the overall software. The requirements are reviewed and adjusted accordingle for each sprint. This allows requirements to change quickly, reducing the delay in obtaining feedback from users and developers to be more responsive to the users' needs.

If there are set of fixed non-negotiable requirments then the waterfall methodolgy is better. For the waterfall methodology each task is planned in advance and carried out in a predetermined order. This ensures that the reuirments are met and at key milestones.
However, the waterfall method lacks flexibility, making it difficult to respond to a change in requirments. Projects that are better suited to the waterfall methodology are those where delivering an intermediate product is not viable.
For example, a library to perform Fourier transformations is not useful unless fully implemented whereas a graphical data processing toolkit could be useful mid way through development.

In reality a mixture of the two is usually implemented. For example consider a graphical user interface (GUI) that has a set of requirments (e.g. load data, do a fit to the data, output results). To make sure that the GUI meets the requirments a waterfall approach could be used. However, within that approach a more agile like methodology could be used. The first two sprints could focus on loading and fitting to data. The third sprint could then focus on making sure they interact with each other correctly. 

\subsection{Coding standards}
\label{Coding standards}

Code should be of a high quality, the reason for this is two fold; code that conforms to accepted standards is more likely to be well formatted, readable and contain fewer bugs.
These factors combine to create a code base that is easier to maintain in the future.

Specific rules depend greatly on the language in question, as such it is better to select a set of standards based on the language your software is written in.
A community curated list of standards organised by language is available online \cite{Awesome_Guidelines}.

An important, language agnostic rule to follow is to write human readable code.
This may be by means of comments, verbose variable names, etc.
Ideally a developer should not have to dig through documentation stored elsewhere or find the correct person to talk to to understand a piece of code.

As an example of this, compare the function \cite{Lim_2015}

\begin{lstlisting}
float calc_v(std::vector<float> y)
{
  float v = 0.0;
  for (int i = 0; i < y.size(); i++) v += (f(y[i]) - y[i]) * s(y[i]) + y[i];
  return v;
}
\end{lstlisting}

against,

\begin{lstlisting}
/**
 * Calculates the linear approximation of pair potentials.
 * @param pair_potentails :: [input] A vector of the pair potential experienced by each atom.
 * @returns The linear approximation of pair potentials.
 */
float linear_approx_of_pair_potentials(
    std::vector<float> const& pair_potentials)
{
  auto lapp{0.0f};
  for (auto const& pair_potential : pair_potentials)
  {
    lapp += ((embedded_atom_potential(pair_potential) - pair_potential) *
             smooth_switch(pair_potential)) +
            pair_potential;
  }
  return lapp;
}
\end{lstlisting}

The improvements in the second code snippet include (any points that are C++ specific are noted):
\begin{itemize}
  \item{A Doxygen comment is present}
  \item{Improved function name that better indicates it's functionality}
  \item{Improved verbose variable names}
  \item{Better use of scope constructs (\texttt{\{} and \texttt{\}}) and newlines}
  \item{Non trivial parameters that are not modified are passed by const reference (C++)}
  \item{Range based for loop is used (C++)}
  \item{Use of \texttt{auto} keyword (C++)}
\end{itemize}

For ease of international collaboration filenames, identifier names, comments and documentation should be published in English.
Translation frameworks may be used to provide your software and documentation in other languages at compile/build time.

To assist in maintaining code quality one or more "linters" (also referred to as "static analysis" tools) should be employed.
These are tools that inspect your code in one way or another to detect potential issues, such as unused variables, indentation, thread misuse, etc.
There are a plethora of linters available for almost every language.
Clang \cite{Clang} and Pylint \cite{Pylint} are two of the most common for C++ and Python respectively.

A lot of tools are available as standalone packages that you simply install, run and produces a report.
Some tools are also available as a cloud service (e.g. Coverity) that is triggered by new code being pushed to your remote repository.

For a wider selection of the available tools it is worth looking over the selection on Awesome Static Analysis (https://github.com/mre/awesome-static-analysis), a community curated list of static analysis tools.

Another general rule to bear in mind when writing new code is to avoid the pitfalls of pre-emptive optimisation, by attempting to reduce execution cost without determining if it is a problem to begin with.
A similar pitfall is pre-emptive generalisation, which is attempting to make the code less problem specific in the hope it can be reused without determining if this affects code quality or if there is even a case where it could be reused.
Of course neither pre-emptive optimisation or generalisation have a concrete definition so it is up to the developer and code reviewers to make informed decisions based on the nature of the work and the code base.

\subsection{Issues and work planning}
\label{Issues and work planning}

A means of tracking work to be done on a software project is essential.
This is often implemented in two parts; low level tracking of implementation tasks and high level tracking of project goals.

For low level task tracking simple tools such as GitHub Issues are likely sufficient, even for large projects.
The essential functionality is providing a description of work to be undertaken and allowing it to be assigned to a developer.
Additional features may include boards such as Scrum or Kanban boards known from e.g.\/ JIRA (TODO provide reference), task dependency and task hierarchy. (TODO: white board with stickers / cards)

Most cloud hosted SCM services provide their own issue tracking tool, which typically suit the needs of most software projects.

If more complex task workflows are needed or if your code base is distributed across multiple locations then it may be beneficial to look into issue tracking separate from your code.

High level tracking, typically at project management level, should track the projects medium to long term goals and their progress.
These goals should be defined by users of the software.

Ideally both levels of tracking should be available to view freely by users and developers.

\subsection{Source control management}
\label{Source control management}

Source control management (SCM) is an essential part of a software project.
This provides a full history of the state of your code base as the project evolves, allows multiple developers to work on the project simultaneously and provides means on reviewing changes to your software.

As with writing code, following a set of guidelines when working with your SCM will allow you to get the most out of it.
As such guidelines apply to a much wider audience there is already ample documentation describing them, some of which are listed at https://github.com/dictcp/awesome-git\#workflow, therefore this section will only give an overview and highlight a few commonly overlooked points.

You must adopt a workflow and enforce it.
A workflow defines how developers interact with the SCM and the process of their changes to the code base being accepted.

When working with developers who may be using SCM for the first time it is common to see unused code (be it commented out code, functions that are never called, files that are not included, etc.) left in the code base.
For the sake of keeping the code base clean it is important to remove such dead code, its presence in the code base only leads to increased difficulty in reading and understanding the code that is actually used and given that history is recorded in the SCM, deleting it comes with no danger.

A popular option, that is also one of the simplest, is the feature branch workflow (see https://www.atlassian.com/git/tutorials/comparing-workflows/feature-branch-workflow).
In this workflow when a developer starts working on a new feature they do so on a new branch and that branch is then merged when the changes are to be accepted.

Developers should also ensure that they produce a clear commit history.
Given that this is likely the longest standing documentation describing the history of your software it is important that commits are sensible, atomic and logically ordered and that commit messages are clear and describe the reasoning behind the changes made in the commit itself.
A good guide for this can be found here: https://chris.beams.io/posts/git-commit/.

Git \cite{Git} is the most common and one of the most powerful SCM tool used in open source software today with a wide variety of training resources available.
It is suitable for a project of any size and will probably last the life of your project.

There are a wide range of cloud Git repository providers, all of which provide mostly similar features.
If your team is distributed then it is recommended to use such a service (opposed to local hosting on site) to avoid network related bottlenecks.

\subsection{Continuous integration}
\label{Continuous integration}

Continuous Integration (CI) should be used, regardless of the projects size, to ensure that your software performs as you expect it to.

CI is essentially a service that detects when new changes have been made to the code base (via the SCM) and runs a set of actions to determine if the code is of good quality.
It can also be used to perform the same checks on proposed changes before they become part of the main code base.

Typically the actions performed include compiling/installing the software and running automated test suites, however any number of additional tasks can be performed including; generating documentation, performing code linting, deploying packages, checking for software vulnerabilities, etc.

For small scale projects that reside in a single repository continuous integration as a hosted service is most likely sufficient.
For example Travis CI \cite{Travis_CI} offer time limited windows for you to run your builds and tests on Linux, Mac OS and Windows operating systems.

Jenkins \cite{Jenkins} is a self hosted CI solution that runs on your own infrastructure.
The key advantage this has is removing restrictions around build time, data availability and specific build environment setup.

\subsection{Code review}
\label{Code review}

\begin{figure}
    \centering
    \includegraphics[height=10cm]{code_review_process.eps}
    \caption{TODO}
    \label{Code_Review_Process}
\end{figure}

Code review is the process of determining if a proposed change should be made to the code base, typically after a developer has finished working on a feature or bug fix.

The depth of the review process depends a great deal on the size and purpose of the project.
The developer proposing the changes should provide a description of their change and instructions on how the tester can demonstrate this.

The tester then must ensure that the code is of good quality and coding styles, guidelines, etc. have been followed, commit history is sane and relevant documentation (including release notes) has been updated.
CI tools can be used to perform some of these checks automatically.

Larger or more critical projects may opt to have multiple review stages, in which additional testers (that has yet to be involved in the development or testing of this proposed change) will perform the same duties as the previous testers.
Having multiple reviewers for the same piece of work may seem inefficient, but will increase the chance of potential issues being found before they are released to users.

Tools for code review are often in built in hosted SCM services, an example of this is GitHub Pull Requests, which allow code to be viewed and commented on.

\section{Architecture}
\label{Architecture}

A well designed software package would separate its functionality into a set of individual "modules" based on common feature areas, where a module may refer to a package, library or software plugin.

Care should be taken when designating these modules, these should contain only functionality relevant to the goal of the module, where the goal itself is a very specific task (e.g. minimization).

Code within a module should be closely related to each other, this reduces importing unnecessary functionality and keeps modules specific to a given purpose.
A benefit gained from making code modular is the ability to test each module in isolation using automated testing (i.e. unit testing).

The code base of your software should be managed using an appropriate build system, this depends on the language (or combination of languages) used.
A good choice for C and C++ projects is CMake \cite{CMake}.
This can handle setting up library paths and build tools automatically, allowing developers on any platform to develop your software with relative ease.
C++ projects can also benefit from use of a dependency manager such as Conan.
Most build systems for other languages combine a dependency manager into the build system, however this is not the case for CMake.
For Python based projects Python's setuptools is typically sufficient, for specific cases (for example, when the software depends on tricky to package dependencies) Conda may be used as an alternative.

If you are making your software extensible by means of a scripting interface this should ideally be done in Python.
Python is becoming the standard language for data science and as such has a wide range of libraries, documentation and resources available for it.

If the application requires a graphical user interface (GUI) then a good choice of library is Qt \cite{Qt}.
Qt is an application framework commonly used by many open source and commercial software packages.
It allows easy development of cross platform GUIs in a variety of languages.
Another increasingly popular option is to provide your software as a web service that is accessed via a web page.

\section{Data Formats and Interoperability}
\label{Data Formats and Interoperability}

It is important for your software to use an open and standardised data format.
Such a data format features open source definitions and documentation, making the format easy to understand and aiding software implementations.

In the neutron science community NeXus \cite{K_nnecke_2015} has become the default format.
NeXus aims to standardise the way similar data is represented by different facilities and software by defining a structure on top of the HDF5 file format \cite{HDF5}.
NeXus also has the advantage of being a self describing file format, where by using the correct classes you make it easy to understand the data with no prior knowledge.
Metadata (such as instrument configurations and data processing history) is trivial to store in NeXus.
Whilst the format has existed for roughly two decades it's widespread use as the standard data format has been a relatively recent effort between facilities and software projects.
One point of note is that when committing to NeXus as your data format, you must ensure you truly implement the standard or publish the definition, if the need to diverge from the standard is genuinely required.
In this case you may evaluate if your changes should form part of the standard and propose such changes.

Proprietary file formats should be avoided at all costs.
While there is sometimes an argument for software specific intermediate formats, they should remain intermediate and NeXus should be the preferred interchange format.

Applications may store information relevant to it as a whole (e.g. window layout for a graphical tool), which should typically be stored using the facilities provided by the application framework (e.g. \texttt{QSettings} for Qt).

Additional configuration or process setup information should be stored in an easily accessible location.
This data may include version numbers, parameters and filenames.
This configuration should ideally be in a human readable plain text format, assuming it's data volume does not negate readability or performance.
Popular formats for this purpose include INI, YAML, JSON, TOML and XML, all of which have several library options for common languages and store data as plain text.
Under no circumstances should custom formats be used for job configuration.
There are already a number of well defined data formats for this purpose, creating a new format would be a waste of developer time.

The ability to reproduce results of data treatment is an important ability for users of your software, to allow this you should provision a mechanism that allows the actions of your software to be "replayed" on the raw data.
To allow this you should provision a mechanism that allows the actions of your software to be "replayed" on the raw data.
Typically this means recording a history of each atomic action your software performs as well as any relevant version numbers (i.e. such as the version of your software, versions of plugins/dependencies).

Process lists in Savu \cite{Wadeson_2016} are a good example of such operation reporting.
These store the job configuration in a NeXus file and encode the same information in the final results (also stored in a NeXus file).
This allows the processing history to remain with the processed data and allows the result dataset to be used as input for a future job.
Savu also provides tooling to allow a user to export a list of citations from their result data which is populated from the process list.

\section{Project Management}
\label{Project Management}

Large pieces of software will require some form of management, to ensure that the team are working effectively.
This is referred to as a governance model.
The scale of the governance model will depend on the size of the project and could range from an advisory committee to a project management board.
A governance model will provide a constructive mechanism for deciding the direction of the project and for ensuring it progresses.

The software should follow the basic notion of a release cycle.
A release is a version of the software that has been tested and the development team is confident that it is ready to be used.
Having periodic releases can also work to reassure users that the project is still supported.
It is advisable to follow semantic versioning \cite{Semantic_Versioning} (major.minor.patch, e.g. 3.1.0) for the naming of the releases.
Prior to a release there will be a "code freeze", where no new code will be submitted to the main branch.
In preparation for a release developers should test the software works as expected with no obvious bugs.
The next stage is "beta testing" where a select group of users are chosen and they test the software and report any bugs.
Once all of the identified bugs are fixed it is then released with more confidence that it is ready for use.

It is advisable to then spend some time working on maintenance tasks, these are improvements to the maintainability of the code rather than any new user facing functionality.
Maintenance tasks are improvements to the maintainability of the code rather than any new user facing functionality.
Examples of such tasks include: moving to a new compiler version, applying fixes for compiler or linter warnings and code reorganisation/refactoring.

When selecting developers for a given task you should consider the skills/knowledge of the developer, the time required to complete the task and the deadline for the task.
If the deadline and time required are similar then it is better to choose a developer who has the relevant skills and experience.
If there is sufficient time between the deadline and the estimated time to complete the task then this provides an opportunity to train a less experienced developer.

\section{Licensing}
\label{Licensing}

All pieces of software require a license.
An open source license is advisable from both a user and development perspective, details of available licenses are available online \cite{OSI_Licenses}.
The users will be able to download and use the software free of charge, removing a potential barrier in the uptake of your software.
The advantage of an open source license is that it allows anyone to contribute to your software (this should not bypass any review process), this radically improves the available pool of developers.
Further in research/academic communities open source is becoming increasingly compulsory in order to ensure that valuable source codes are not lost.

\section{Packaging and Distribution}
\label{Packaging and Distribution}

\subsection{Releases}
\label{Releases}

\begin{figure}
    \centering
    \includegraphics[height=10cm]{release_workflow.eps}
    \caption{TODO}
    \label{Release_Flowchart}
\end{figure}

A good release process depends greatly on the nature of the software project.
A large framework used by multiple different scientific techniques at different facilities (such as the Mantid project) requires an involved release process to ensure the software is suitable for use by all intended audiences.

This section will assume you have a substantial number of users that you do not directly interact with during your development process.

In order for any potential issues in your software to be found and rectified you may wish to create a release candidates of your software before the official release.
These versions are intended for "power users" who do not mind testing your software for the benefit of the wider audience of your software (a typical example at a large scale research facility is a scientist responsible for an instrument and its user programme, and who wants to ensure your software works before providing it to their visiting users).

The length of the beta period is determined by your release cycle and your estimated number of users.
There is no generic rule for this, but it is recommended that you ensure all (or as close to as possible) use cases of your software have been tested by users who specialise in that area.

There can be any number of release candidates.
As the name suggests a release candidate should be generated in exactly the same manner as a release (the only difference being the version number).

To ensure users approach the new version with realistic expectations it is essential to provide sufficiently detailed release notes with every version of your software (release, beta, release candidate).
These should summarise the changes made to your software since the last release, possibly highlighting some specific new features (this is where you "sell" your new version and give a reason for users to upgrade).

It is essential that all changes that may break existing workflows are listed.
This will prevent issues when users attempt to use incompatible existing workflows with your new version.

Any fixes for safety or security reasons must be clearly documented in the release notes.

It is recommended to tag and sign the release in your SCM system, in the case of Git this involves creating a tag from the code that was used to generate your release package and signing it with either a project or maintainers PGP (Pretty Good Privacy) key.
This is not an essential step, but is good practice to allow verification of historic releases.

It is also recommended to generate a Digital Object Identifier (DOI) for each release of a piece of data treatment software.
This allows your software to be easily cited in publications and makes it clear which version of your software was used to generate results in a given publication.
Another method is to use a publication that provides the idea of the software.

\subsection{Packaging}
\label{Packaging}

To ensure user satisfaction easy software setup is essential.
Exactly what this involves varies depending on the nature of your project, as such this section will not go into great detail about specific options, but rather the requirements of an installer and process of choosing an appropriate installation method for your software.

The first requirement is for the package to give users a working copy of your software.
They should not have to care about any implementation specifics or having to install additional dependencies to make your software run.

Once installed, the software should not conflict with any other software a user may have installed on their machine.
A common example of this is when applications depend on the same libraries, such as HDF5.

It is recommended to sign binaries, this allows a user to verify that the copy of the software they have is in fact the version they believe it is and that it has not been modified.
Some operating systems may also impose restrictions on running unsigned binaries.
How binary signing is done depends on the packaging method and target operating system, it is worth researching this for your specific case.

As the technology becomes more popular it is worth looking into cross platform container based means of distribution (for example, Docker \cite{Docker}, Singularity \cite{Kurtzer_2017} and other container based systems).
This allows the distribution of software in an already configured and working state being safe in the knowledge it will function in the same way on the user's computer.
Different containerization systems have their own caveats and there is no "one size fits all" solution.
For example, Singularity is better at packaging GUI software than Docker, however Docker images are likely to more universally usable due to Docker's popularity.

\subsection{Cross platform}
\label{Cross platform}

Unless your software specifically targets high performance computing systems such as clusters or distributed compute platforms then no assumption can realistically be made about how, and specifically on what platform, a user will expect to run your software.
For this reason it is highly desirable (or even essential) that your software is designed to run on multiple operating systems and architectures.
Thanks to modern development tools, libraries and build systems this is not a complex task.
By using cross platform libraries you also allow your software to be easily adopted by users that use an operating system or environment different to your own.

If officially supporting multiple platforms then it is beneficial to encourage a spread of different platforms and operating systems across your development team.
Doing so ensures that there are always multiple people capable of addressing platform specific issues when they arise.

It is recommended to limit the versions of particular operating systems you support.
Typically you should stop supporting an OS version when the publisher stops supporting it.

\subsection{Dependency Management}
\label{Dependency Management}

Dependency management handles obtaining and configuring any library dependencies your software has at build/configure time.
This serves two functions: to make the build environment easy to set up and to make builds reproducible.
Dependency managers do this by collecting requirements for your software based on a specification file provided by the developer.
In the case where a package manager is available (for example Conda \cite{Conda}, PyPI \cite{PyPI} or Conan \cite{Conan}) this may just be a package name and version number.
For more general purpose tools it may involve instructions on how the dependency should be retrieved and configured (for example CMake's ExternalProject \cite{CMake_ExternalProject}).

One of the benefits of dependency management is the ability to pin dependency versions to known working versions.
This ensures that moving to new development or runtime environments will not cause incompatibilities between your software and software it depends on.
It is however important to keep dependencies up to date to ensure known bugs are not included in your software.
The task of updating dependency versions and fixing any issues that arise is a good candidate for a maintenance task (see section \ref{Project Management}).

\section{Support and Documentation}
\label{Support and Documentation}

Good documentation will prevent the developers having to spend time explaining how to use and develop the software to users.
The hallmarks of good documentation are that it is easily available, concise and written in a way that is accessible to the target audience.

The simplest and most common form is online documentation, for example Read the Docs \cite{Read_The_Docs} or a wiki page.
The documentation should have a clear structure and be easy to navigate.
An easy way to achieve this is to have a contents page and to make the pages searchable so users can easily find the relevant information, by having tags that summarise the main themes of each page.
The online documentation should be version controlled so it is possible to revert changes and to follow its evolution.

Projects may benefit from having a landing page introducing the project and directing users to commonly used or important resources.
Examples for this are Hugo \cite{Hugo} and GitHub Pages \cite{GitHub_Pages}.
Hugo is a powerful and fast static website generator.
It supports themeing and customisation and works with content defined in Markdown.
A free hosting service provided by GitHub Pages, this allows you to commit your website content to a specific branch and have it served by GitHub.
The large benefit here is avoiding the cost and time of maintaining the infrastructure required to host the project's website.

An increasingly popular method of documentation is the video tutorial.
These are extremely useful to users as it allows them to see how the software is used.
For example, video tutorials of intricate GUIs may be beneficial as the user gets to see a complete workflow and there is less chance of steps being undocumented.
However video tutorials can be costly to create and keep up-to-date with software changes.

Face to face communication is often the most efficient and effective way to understand a user's question/comment.
This allows for a conversation to occur with more probing questions to improve the developer's understanding and to overcome any unfamiliar terminology.
As a result it is important to have some local support, where developers are preferably in close proximity to their users.
Face to face contact also has the benefit of increasing the user experience and users are more likely to report problems/bugs to the development team if they can easily speak to them.

A key activity for any piece of software is the user support.
Unlike documentation this is more interactive between developers and users.
As a minimum email support should be provided and users made aware of it.
The emails should be checked regularly and initial responses should be within a day or two.
The initial response may be to ask for more details, to arrange a meeting or to say that it is being worked on.
Providing effective user support will give users more confidence in the software.

The documentation discussed so far has focused on the users.
It is also valuable to have developer documentation.
This documentation should contain information on the standards used in the code and how to set up a development environment, appropriate tutorials and relevant reference material.

\section{SINE2020}
\label{SINE2020}

TODO

\section{Conclusion}
\label{Conclusion}

The implementation of software standards and guidelines to a computational project has numerous benefits.
It fundamentally allows for the software to be developed and maintained easily by a distributed team, while providing support to users.
There is an initial cost in deciding what the standards/guidelines should be and they should be chosen to based on the requirements of the project.
To ensure the smooth running of the project a governance model should also be implemented.
It is important to follow the standards and processes that have been set out and these may need updating for long life projects.
To ensure users have a positive experience it is important to provide support and to have a release cycle.

Standards and guidelines ensure a uniform code quality, a structured developer team and an improved user experience.
These factors will help create a stable, maintainable piece of software and the likelihood of the software's user base flourishing.

\section{Acknowledgements}
\label{Acknowledgements}

Thanks to Ed Beard for initial proofreading.

This work was funded by the Horizon 2020 Framework Programme of the European Union. Project number 654000.

%%%%%%%%%%% The bibliography starts:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  ios1.bst will be used to                               %%
%%  create a .BBL file for submission.                     %%
%%                                                         %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\nocite{*}
\bibliographystyle{ios1}
\bibliography{bibliography}

\end{document}
