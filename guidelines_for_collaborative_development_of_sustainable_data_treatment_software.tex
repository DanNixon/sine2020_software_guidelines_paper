% add. options: [seceqn,secthm,crcready,onecolumn]
\documentclass[jnr]{iosart2x}

%\usepackage{dcolumn}
%\usepackage{endnotes}

%%%%%%%%%%% Put your definitions here

\usepackage{listings}
\usepackage{rotating}

\definecolor{coral}{RGB}{255,55,0}

\lstset{
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}; should come as last argument
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{coral},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=C++,                 % the language of the code
  morekeywords={*,...},            % if you want to add more keywords to the set
  numbers=none,                    % where to put the line-numbers; possible values are (none, left, right)
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mauve},     % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}

%%%%%%%%%%% End of definitions

\pubyear{0000}
\volume{0}
\firstpage{1}
\lastpage{1}

\newcommand{\todo}[1]{\textbf{#1}}

\begin{document}

\begin{frontmatter}

%\pretitle{}
\title{Guidelines for collaborative development of sustainable data treatment software}
\runningtitle{Guidelines for collaborative development of sustainable data treatment software}
%\subtitle{}

% Two or more authors:
\author[A]{\inits{D.}\fnms{Daniel} \snm{Nixon}\ead[label=e1]{daniel.nixon@stfc.ac.uk}%
\thanks{Corresponding author. \printead{e1}.}},
\author[A]{\inits{A.}\fnms{Anders} \snm{Markvardsen}\ead[label=e3]{anders.markvardsen@stfc.ac.uk}},
\author[C]{\inits{A.}\fnms{Anders} \snm{Kaestner}\ead[label=e5]{anders.kaestner@psi.ch}},
\author[D]{\inits{J.}\fnms{Joachim} \snm{Wuttke}\ead[label=e6]{j.wuttke@fz-juelich.de}},
\author[A]{\inits{S.}\fnms{Stephen} \snm{Cottrell}\ead[label=e7]{stephen.cottrell@stfc.ac.uk}},
\author[E]{\inits{M.}\fnms{Miguel} \snm{Gonzalez}\ead[label=e8]{gonzalezm@ill.fr}},
\author[A]{\inits{A.}\fnms{Anthony} \snm{Lim}\ead[label=e2]{anthony.lim@stfc.ac.uk}},
and
\author[B]{\inits{T.}\fnms{Thomas} \snm{Holm Rod}\ead[label=e4]{Thomas.HolmRod@esss.se}}

\runningauthor{D. Nixon et al.}
\address[A]{\orgname{Science and Technology Facilities Council},\cny{United Kingdom}\printead[presep={\\}]{e1,e2,e3,e7}}
\address[B]{\orgname{European Spallation Source},\cny{Sweden}\printead[presep={\\}]{e4}}
\address[C]{\orgname{Paul Scherrer Institute},\cny{Switzerland}\printead[presep={\\}]{e5}}
\address[D]{\orgname{Forschungszentrum JÃ¼lich},\cny{Germany}\printead[presep={\\}]{e6}}
\address[E]{\orgname{Institut Laue-Langevin},\cny{France}\printead[presep={\\}]{e8}}

\begin{abstract}
Scientific software is an essential part of the modern researcher's toolkit.
Due to the complex analysis techniques required, it is often expensive to develop and maintain scientific software.
Therefore, it is imperative to ensure that the software will continue to be supported in the future, even if key staff leave the project.
Some of the requirements for sustainable software are for the code base to have a consistent quality and good documentation, to prevent a loss of knowledge.
This paper presents two sets of guidelines.
The first is a generic set that could be applied to software projects across a broad range of domains.
The second set of guidelines are specific to the neutron scattering and muon spectroscopy data treatment communities and were developed as part of Science and Innovation with Neutrons in Europe in 2020 (SINE2020).
These specific guidelines were designed to allow the harmonization of software development process.
Software outside of the neutron and muon communities can use the specific guidelines as inspiration of how to adopt, apply and adapt the generic guidelines presented in this paper.
\end{abstract}

\begin{keyword}
\kwd{Software development}
\kwd{Scientific software}
\kwd{Sustainable software}
\kwd{Data treatment}
\kwd{Neutron scattering}
\kwd{Muon spectroscopy}
\end{keyword}

\end{frontmatter}

%%%%%%%%%%% The article body starts:
\newpage

\section{Introduction}
\label{Introduction}

It has become vital for modern neutron and muon sources to ensure that excellent software is available for the user community.
In order to ensure efficient production of scientific results and publications from experiments it is important for facility software to be of a high standard.
The reason for this is two-fold: first, the demographic of the users is changing to scientists increasingly using neutron scattering or muon spectroscopy as one of many tools in addressing their scientific problem rather than specialising in the scientific techniques themselves..
Hence, an increasing number of users are not knowledgeable about the details behind the neutron and muon data treatment and  do not have the technical skills in how to treat and analyse their data.
The second reason is that the advances in science are generating increasingly large datasets that cannot be processed by hand.
For example, some instruments at the European Spallation Source ESS) are estimated to have data rates in the order of gigabytes per second \cite{Christensen_2018}, which is significantly higher than at existing neutron sources.

The growing complexity of data treatment means that the development of scientific software requires ever greater resourcing.
The consequence of these changes is that users will inherently depend on software created by other developers, typically in the user community or at the hosting facilities.
A person may also choose to contribute to existing software rather than develop their own from scratch.
This adds significant requirements in terms of quality, trustability, reliability, reproducibility and usability compared to when users develop software for their own use.
(TODO: insert \cite{computational-science-error} and articles about reproducibility)
It also adds a requirement of interoperability to the software developed by one group (e.g. at a university) so that it can be used by users and developers (e.g. at a facility).
New methods for data treatment are often developed in collaboration with academia.
Therefore, it would be beneficial to the broader community if the new methods could be easily incorporated into facilities' software.
A potential pitfall with this isolated approach of software development is that bugs are less likely to be found, due to fewer users.
Also the software may become too cumbersome to develop leading to the project having to start from scratch.
These factors make it more difficult to share software and for other research groups to adapt the software for their own purpose.

The lack of guidelines can lead to the code being difficult to read, because the style will change depending on who wrote it.
In the worst case this may lead to people believing that the code is incorrect, when it is correct and vica-versa.

Guidelines for software development ensure that the code is of a high quality and that the project progresses effectively.
A good set of guidelines will help the software be sustainable; that is,
\begin{itemize}
\item{reliable}
\item{inter-operable}
\item{extensible}
\item{intuitive}
\item{maintainable.}
\end{itemize}

With these in place it will be easier to continue growing the code base to include new functionality to the current scientific analysis techniques and to expand the software to serve new areas of analysis/research.
It would also reduce the effort required to adapt the software for use at other facilities.

There is plenty of literature discussing different aspects of software development.
Initiatives such as CodeRefinery \cite{coderefinery} and the Software Sustainability Institute \cite{SoftwareSustainabilityInstitute} promote best practises for the development of academic software \cite{fair_software}.
With the move towards FAIR (Findable, Accessible, Inter-operable and Reusable) \cite{panosc} data, concerns about the reproducibility and reliability of data has become even more important to scientists.
These concerns are in part rooted in the rapid replacement of developers, in the form of PhD students and post-docs.
These developers may work in isolated conditions with limited knowledge of previous developments and the reasoning for them.
However, some academic groups and research infrastructures are now starting to follow guidelines when writing software.

The aim of this paper is to define good software practices and evidence how these can benefit the scientific software community.
Each individual community may use different guidelines, some of which may already exist and others can be specific to a particular project.
In section \ref{SINE2020}, guidelines are presented that were developed as part of \underline{S}cience and \underline{I}nnovation with \underline{N}eutrons in \underline{E}urope in \underline{2020} (SINE2020, \cite{sine2020}).
The aim of these guidelines was to foster development of high-quality software, collaborations and contributions from third party developers (e.g. from university groups) through the development of inter-operable software.
If the guidelines are followed, it will be easier for (another) facility to adapt the software for their own needs and that of their user community, but also for other developers to contribute to the software.

The specific guidelines for the neutron and muon communities are based on an analysis of existing practice for software considered in SINE2020, namely {\it BornAgain} \cite{bornagain}, {\it Mantid} \cite{mantid}, {\it ImagingSuite} \cite{ImagingSuite}, {\it McStas} \cite{mcstas} and {\it SasView} \cite{sasview}.
These projects have a wide range of backgrounds.
For example, some are maintained by an individual and others by large international teams (see Section \ref{SINE2020}).

To put those rather specific guidelines in context we start with a presentation and discussion of some more generic guidelines along with a discussion of challenges commonly faced in software development that these guidelines address.

Although the specific guidelines are aimed at the neutron and muon communities, the generic guidelines reflect what is common practice in software development.
When it is appropriate the idea of the guideline will be abstracted, so that an alternative can be chosen in the future.

Abstracting guidelines in this manner allows them to remain relevant as technology evolves.

\todo{find me a home, and make me make sense}
Guidelines have been developed within other communities to the benefit of the project.

\section{Data Formats and Interoperability}
\label{Data Formats and Interoperability}

It is important to use an open and standardised data format, in order to allow users to easily create suitable input files for the software.
A good data format will have open source definitions and documentation, making it easy to understand and to implement file loaders for the software.
It is likely that a library will already exist for loading standardised file formats and should be used whenever possible.
Any implementation of a standardised data format needs to follow the standard and
the data format should only diverge from it if it is absolutely necessary.
If the divergence from the standard is required and has a clear benefit, then these changes should be suggested to be added to the standard.

In the neutron and (some) muon communities {\it NeXus} \cite{K_nnecke_2015} has become the default format for raw data and processed data.
If a particular scientific community already has an accepted standard that should be used.
{\it NeXus} aims to standardise the way data is represented by different facilities and software by defining a structure on top of the HDF5 file format \cite{HDF5}.
{\it NeXus} is a self describing file format where classes can be used to make it easy to understand the data, with no prior knowledge.
Useful metadata is trivial to store in NeXus and is an important aspect for making data FAIR.
Correctly labeled metadata significantly improves the ability to use a search query in a data catalogue.
Well documented metadata improves the reproducibility and reusability of the data.
Standardized metadata will improve how easy it is to identify key information.
Examples of metadata are instrument configurations, data processing history, sample environment setting, sample description, experiment proposal etc.

Proprietary file formats should be avoided at all costs.
While there is sometimes an argument for software specific intermediate formats, they should remain intermediate.

Applications may store relevant information (e.g. window layout for a graphical tool) using the facilities provided by the application framework (e.g. \texttt{QSettings} for Qt).

Additional configuration or process setup information should be stored in an easily accessible location.
This data may include version numbers, parameters and filenames.
The configuration file should be in a human readable plain text format, assuming it's data volume does not negate readability or performance.
Popular formats for this purpose include INI, YAML, JSON, TOML and XML, all of which have several library options for common languages and store data as plain text.
Under no circumstances should custom formats be used for configuration files, because there are already a number of well defined data formats for this purpose.

It is vital for the results from scientific software to be reproducible.
However, sometimes there is a need to recreate the results from data treatment, in this case it becomes valuable to be able to replay the series of operations.
Data storage is a concern for most facilities and being able to reproduce data can be beneficial.
For particularly large datasets an alternative method is to store the unprocessed data and instructions on how to process the data.
This means recording a history of each atomic action the software performs along with any relevant numbers/inputs.
Therefore, it is possible to record multiple sets of instructions with a negligible increase in storage space.

It is essential to be able to extract the exact conditions of the software (i.e. the version number of the software, versions of plugins/dependencies).
Process lists in {\it Savu} \cite{Wadeson_2016} are a good example of such operation reporting.
These store the job configuration in a NeXus file and encode the same information in the final results.
This allows the processing history to remain with the processed data and allows the result dataset to be used as input for a future job.
Savu also provides tooling to allow a user to export a list of citations from their result data and is generated from the process list.

\section{The daily development work}
\label{Business as Usual}

This section discusses the day-to-day work of software developers (i.e. development methodologies, code review and coding standards).
Another important part of the developers' job is to provide documentation for users and other developers (see Section \ref{Documentation}).
This reduces the risk of having single points of failure, because another developer can read the documentation and then continue the work.
Most of these choices will be made at the start of the project and ideally last its duration; however, these can be updated to accommodate new tools and standards.

\subsection{Development Methodology}
\label{Development methodology}

Development methodology is the set of rules that determine how requirements are gathered, converted into developer tasks and how the software is delivered to the user.

The two extremes of software development methodologies are called "agile" and "waterfall".
In the following we will discuss these two extremes; however, aspects of these two methodologies can be used simultaneously. \todo{add ref}

For scientific software agile, or some variant of that, is the most common methodology.
The general principle of the agile methodology are short development "sprints" in which small, incremental additions are made to the overall software.
If the sprint has a predetermined duration (usually a few weeks) then it is referred to as Scrum.
The requirements are reviewed and adjusted accordingly for each sprint.
This allows requirements to change quickly, reducing the delay in obtaining feedback from users, allowing the development cycle to be more responsive to the users' needs.

If there are set of fixed non-negotiable requirements then the waterfall methodology is better.
For the waterfall methodology each task is planned in advance and carried out in a predetermined order.
This ensures that the requirements are met and at key milestones.
However, the waterfall method lacks flexibility, making it difficult to respond to a change in requirements.
This potentially means that correcting a mistake in the design, later on in the project can be costly.
Projects that are better suited to the waterfall methodology are those where delivering an intermediate product is not viable.
For example, a library to perform Fourier transformations is not useful unless fully implemented whereas a graphical data processing toolkit could be useful mid way through development.

In reality a mixture of the two is usually implemented.
One potential way to do this is to have agile development linked to a long term plans with high-level requirements and milestones, e.g. a graphical user interface (GUI) that has a set of high level requirements (e.g. load data, do a fit to the data, output results).
To make sure that the GUI meets the high level requirements a waterfall approach could be used.
However, within that approach a more agile like methodology could be used.
The first two sprints could focus on loading and fitting to data.
The third sprint could then focus on making sure they interact with each other correctly.

\subsection{Requirement Capture}
\label{Requirements}

\todo{TODO}

\subsection{Architecture and Design}
\label{Architecture}

\todo{mention SOLID somewhere}

A well designed software package would separate its functionality into a set of individual modules based on common feature areas.
A module may refer to a package, library or software plugin.

Care should be taken when designing these modules.
Each module should perform a very specific task (e.g. minimization) and should only contain the relevant functionality.
Code within a module should be closely related, reducing the need to import unnecessary functionality.
It should be possible to test each module in isolation using automated testing (see section \ref{Quality assurance}), reducing the risk of bugs being introduced into the code base.

It can be desirable to allow users to write scripts to further extend the functionality of the software.
Ideally any scripting made available to users should use the {\it Python} language.
Python is becoming the standard language for data science and as such has a wide range of libraries, documentation and resources are available for it \cite{python-nature}.
Some of the most commonly used libraries are linked from SciPy.org \cite{scipy}.
Python also works fluently with the {\it Jupyter notebook}, an increasingly popular tool in the scientific community \cite{jupyter-nature}.

If the application requires a GUI then a good choice of library is {\it Qt} \cite{Qt}.
{\it Qt} is an application framework commonly used by many open source and commercial software packages.
It allows easy development of cross platform GUIs in a variety of languages (e.g. {\it C++} and {\it Python}).
Another increasingly popular option is to provide a web based interface.

\subsection{Coding Standards}
\label{Coding standards}

To create a code base that is maintainable into the future code should be of a high standard and conform to accepted standards.
Such code is more likely to be well formatted, readable and contain fewer bugs.

Coding standards should be chosen according to the programming language(s) being used.
If multiple languages are going to be used it is possible to have rules for each language.
A community curated list of standards organised by language is available online at \cite{Awesome_Guidelines}.

An important, language agnostic rule to follow is to write human readable code.
For example, use comments to explain the code and descriptive variable names.
A developer should be able to understand what the code is doing (but not necessarily why) without looking at the documentation or talking to the previous author.

For example, compare the function \cite{Lim_2015} written in {\it C++11}

\begin{lstlisting}[frame=single, language=C++]
float calc_v(std::vector<float> y)
{
  float v = 0.0;
  for (int i = 0; i < y.size(); i++) v += (f(y[i]) - y[i]) * s(y[i]) + y[i];
  return v;
}
\end{lstlisting}
against,
\begin{lstlisting}[frame=single, language=C++]
/**
 * Calculates the linear approximation of pair potentials.
 * @param pair_potentails :: [input] A vector of the pair potential experienced by each atom.
 * @returns The linear approximation of pair potentials.
 */
float linear_approx_of_pair_potentials(
    std::vector<float> const& pair_potentials)
{
  auto lapp{0.0f};
  for (auto const& pair_potential : pair_potentials)
  {
    lapp += ((embedded_atom_potential(pair_potential) - pair_potential) *
             smooth_switch(pair_potential)) +
            pair_potential;
  }
  return lapp;
}
\end{lstlisting}

the improvements in the second code snippet include (any points that are {\it C++} specific are noted):
\begin{itemize}
  \item{A {\it Doxygen} \cite{doxygen} comment is present.}
  \item{A more descriptive function name, which indicates it's purpose.}
  \item{Improved verbose variable names.}
  \item{Better use of scope constructs (\texttt{\{} and \texttt{\}}) and newlines.}
  \item{Non trivial parameters that are not modified are passed by const reference (\{it C++}).
  \item{A range based for loop ({\it C++}).}
  \item{Use of \texttt{auto} keyword ({\it C++}).}
\end{itemize}

For ease of international collaboration filenames, identifier names, comments and documentation should be published in a single language (usually English).
Translation frameworks may be used to provide your software and documentation in other languages at compile/build time.

To assist in maintaining code quality one or more "linters" (also referred to as "static analysis" tools) should be employed.
These are tools that inspect the code base to detect potential issues, such as unused variables, indentation errors, thread misuse etc.
There are linters available for almost every language.
For example, {\it clang-format} \cite{Clang} and {\it Pylint} \cite{Pylint} are two of the most common for {\it C++} and {\it Python} respectively.
Linters are available as part of standalone packages that produce result reports, these are typically easy to install and run.
Some of these packages are also available as a cloud service (e.g. Coverity Scan \cite{coverity}), when new code is added to the remote repository the package will run and produce a result report.
For a wider selection of the available tools it is worth looking over the selection on Awesome Static Analysis \cite{awesome_static_analysis}, a community curated list of static analysis tools.

When writing new code care should be taken to avoid pre-emptive optimisation, where the speed up in the execution time may not be significant.
A similar pitfall is pre-emptive generalisation, which is attempting to make the code less problem specific in the hope it can be reused later.
In both of these cases it is better to first determine if any additional optimisation or generalisation is genuinely beneficial.
Of course neither pre-emptive optimisation or generalisation have a concrete definition so it is up to the developer and code reviewers to make informed decisions based on the nature of the work and the code base.

\subsection{Issue Tracking and Work Planning}
\label{Issue tracking and work planning}

Tracking the tasks for a software project is essential.
This is often implemented in two parts; low level tracking of implementation tasks and high level tracking of project goals.

For low level task tracking, simple tools such as GitHub Issues are likely sufficient.
The purpose of issue tracking is to provide a description of the task and to assign it to a developer.
Additional features may include task boards (e.g. Scrum or Kanban boards), task dependency and task hierarchy, which can also be used for high level tracking.

Most cloud hosted Source Control Management (SCM) services provide their own issue tracking tool, which typically suit the needs of most software projects.

If more complex task workflows are needed, or if your code base is distributed across multiple locations, then it may be beneficial to separate issue tracking and the code base.

High level tracking, typically at project management level, should monitor the project's medium to long term goals and their progress.
In the case of functional requirements, these goals should be defined by users of the software, see Section \ref{se}.

Ideally both levels of tracking should be available to view freely by users and developers.

\subsection{Source Control Management (SCM)}
\label{Source control management}

SCM provides a full history of the code base, allows multiple developers to work on the project simultaneously and provides means on reviewing changes to your software.
Hence, SCM is essential for software projects.
However, to fully utilise the benefits of SCM, it is necessary to have well defined guidelines.
In fact, there are ample guidelines for SCM, for more information please see Ref. \cite{awesome-git}.
Therefore, this section will only give an overview and highlight a few commonly overlooked points.

A workflow defines how developers interact with the SCM.
This includes the process for reviewing and accepting changes into the code base.
As a result it is important to define a workflow and enforce it to ensure code quality.

It is common for less experienced developers to try and submit unused code (e.g. commented out code, functions that are never called and files that are not included) to the code base.
It is important not to add dead code into the code base, because its presence reduces readability.
Since the history of the code base is recorded as part of the SCM, removing dead code does not risk losing important information.

A simple and popular option is the feature branch workflow \cite{feature-branch}.
In this workflow a developer will work on a new feature using a new branch and that branch is then merged when the changes are accepted.

Developers should also ensure that they produce a clear commit history.
Given that this is likely to be the longest standing documentation, describing the history of the code base is important.
The commits should be sensible, atomic and logically ordered.
The commit messages need to be clear and describe the reasoning behind the changes made in the commit.
For further reading please see Ref. \cite{git-commit}.

{\it Git} \cite{Git} is the most common and one of the most powerful SCM tools used in open source software.
One advantage is that it has a wide variety of training resources available (see e.g. Refs. \cite{github-git, atlassian-git, codecademy-git}).
{\it Git} is suitable for a project of any size and will probably last the full life of the project.

There are a wide range of cloud Git repository providers, all of which provide similar features.
For a distributed team it is beneficial to use a cloud {\it Git} repository (as opposed to local hosting on site) to avoid network related bottlenecks.

\subsection{Build Systems}
\label{Build systems}

The build and packaging of your software should be facilitated by an appropriate build system, which will depend on the choice of programming language(s).
A good choice for {\it C} and {\it C++} projects is {\it CMake} \cite{CMake}.
{\it C} and {\it C++} projects can also benefit from use of a dependency manager such as {\it Conan} \cite{Conan}.
There are some build systems that combine a dependency manager into them.
For {\it Python} based projects {\it Python's} {\it setuptools} is typically sufficient, for specific cases (e.g. complex package dependencies) {\it Conda} may be used as an alternative.
Most languages will either be shipped with a build system or have a recommended set of tools.

\subsection{Quality Assurance (QA)}
\label{Quality assurance}

The quality of the software is one of the most important factors influencing the long-term success of the project.
Where possible this should be provided through automated testing, i.e. tests which are defined in code and are able to give a pass or fail result without any human interaction.
Apart from unit tests aiming to test the functionality of small units of code in isolation, integration tests are also necessary to ensure that the different parts of the software work together as expected.
Deployment tests ensure that the software is properly packaged and can be installed and launched on all the supported platforms.

It is a good practice to add a test case each time a bug has been revealed in a certain usage scenario.
In such circumstances it is advised to first write the test revealing the bug, and then fix the faulty logic in order to pass the test (Test Driven Development \todo{cite plz}).
This guarantees that the same bug will not reoccur.

Any software that performs computationally heavy operations might need performance tests.
They do not necessarily check the output, but they record the execution time.
By monitoring the evolution of the timings of the performance test results, it is possible to automatically detect when a new code change causes a significant increase in the execution time.
The performance tests can be split into two categories: micro benchmarking and integration performance tests.
The former test the performance of small sections of code to prevent possible flaws, for example, related to caching, threading, database or network access, etc.
The latter test certain usage scenarios over sufficiently large datasets in order to mimic the reaction time  experienced by the end users.

When it comes to automated testing, the GUI is arguably the layer that is the hardest to test.
However, there are techniques and tools that allow sufficient automation of GUI testing.
Among those is the separation of the logic into tiers, e.g. Model-View-Presenter (MVP), where the model and the presenter can be covered by unit tests with mocking.
Mocking is a technique in which a specific part of a codebase is tested by replacing components with which it interacts with "mock" components.
This removes the dependency on other parts of the software and allows components of a complex system to be tested in isolation.
A common place this is done is the presenter layer of an MVP GUI.
There are tools available that allow automated tests of the views themselves (\todo{example}).

Automated testing infrastructure is essential in order to provide a reliable software product.
There is a plethora of testing frameworks available nowadays; almost every modern programming language provides a unit testing framework (e.g. {\it unittest} for {\it Python}, {\it JUnit} for {\it Java}).
However, there are more advanced third party testing frameworks (such as {\it Pytest}, {\it GoogleTest} or {\it GoogleMock}) that provide extended techniques for testing, for example mocking.

While automated testing is indispensable for any software project, the importance of manual (both scripted and unscripted) testing should not be underestimated.
This should be done periodically by the developers or a dedicated QA team, typically before the releases or other milestones.
These tests help detect anomalies that haven't been (or couldn't have been) covered by the automated tests.
It is advised to have readily available recipes that the tester can follow during testing.

\todo{move and rewrite?}
One peculiarity of experimental data treatment software is that it is particularly hard to test.
Given the noisiness and stochastic nature of the input data, often empirical methods and heuristics put under the processing logic make it hard to predict the expected output under all possible scenarios.
For example, an algorithm that is supposed to find a peak may operate as expected if there is a well defined peak in the data, but may fail for low statistics measurements.
Therefore, having unit and integration tests with the exact expected outputs is often not feasible.
In such cases reproducibility tests can be used.
These tests ensure that the output remains reproducible over time, but do not test the correctness of the output of the code.
The purpose of these tests is to prevent unexpected changes in the behaviour of the software as a side effect of other development work.
They exploit reference data to compare the output to and the reference itself should be updated only in the case of bug fixes.

\subsection{Validation}
\label{Validation}

\todo{TODO}

\subsection{Continuous Integration (CI)}
\label{Continuous integration}

Continuous Integration (CI) should be used, regardless of the project's size, to ensure that the software performs as expected.

CI is essentially a service that detects when new changes have been made to the code base (via the SCM) and runs a set of actions to determine if the code is of an acceptable quality.
It can also be used to perform the same checks on proposed changes before they become part of the main code base.

The obvious action is to compile/install the software and to run automated tests.
However, any number of additional tasks can be performed, including generating documentation, performing code linting, deploying packages and checking for software vulnerabilities.

For most projects, using a hosted service for continuous integration is probably sufficient.
For example, Microsoft Azure DevOps \todo{cite} offers a free service that will compile builds and run tests over a fixed time limit (available for {\it Linux}, {\it Mac OS} and {\it Windows}).

For projects with specific CI requirements (such as large build times, the need for specialist hardware or access to data) an entirely self hosted solution should be considered.
For example, {\it Jenkins} \cite{Jenkins} is a self hosted CI solution that requires the user to provide the infrastructure.
As a result {\it Jenkins} can be adapted to suit the specific requirements of the software project.

\subsection{Code Review}
\label{Code review}

Code reviews are the process of determining if a proposed change should be added to the code base.
Typically this occurs after a developer has finished working on a feature or bug fix.
The depth of the review process depends on the size and purpose of the project.
The developer proposing the changes should provide instructions on how to test if the changes are successful and why they are necessary.
The reviewer must ensure that the code is of an acceptable quality (coding styles, guidelines, etc. have been followed).
This should also include if the commit history is sensible and the relevant documentation (including release notes) has been updated.
CI tools can be used to perform some of these checks automatically.

Larger or more critical projects may opt to have multiple review stages, in which additional reviewers will perform the same duties as the previous reviewer.
Having multiple reviewers for the same piece of work may seem inefficient, but will increase the chance of potential issues being found before they are added to the code base.

Tools for code reviews are often built into hosted SCM services.
For example, GitHub Pull Requests allow code changes to be viewed and commented on.

\begin{figure}
    \centering
    \includegraphics[height=10cm]{code_review_process.eps}
    \caption{This flowchart gives an example of a code review process that may be used in a collaborative project.}
    \label{Code_Review_Process}
\end{figure}

\subsection{User and Developer Documentations}
\label{Documentation}

\todo{mention different audiences for documentation}

Good documentation will minimize the time developers spend explaining the software.
Ideal documentation will reduce this time is zero.
The hallmarks of good documentation are that it is easily available, concise and written in a way that is accessible to the target audience.

The simplest and most common form is online documentation, for example Read the Docs \cite{Read_The_Docs} or a wiki page.
The documentation should have a clear structure and be easy to navigate.
An easy way to achieve this is to have a contents page and to make the pages searchable so users can easily find the relevant information, by having tags that summarise the main themes of each page.
The online documentation should be version controlled so it is possible to revert changes and to follow its evolution.

Projects may benefit from having a landing page introducing the project and directing users to commonly used or important resources.
{\it Hugo} \cite{Hugo} and {\it Jekyll} \cite{Jekyll} are examples of popular and powerful tools for creating static web sites.
They support themes and customisation and works with content defined in Markdown.
Many of the cloud based hosting providers have associated free hosting services, which allow you to commit your website content to a specific branch and have it served by the hosting provider.
Popular examples are GitHub Pages \cite{GitHub_Pages} and GitLab Pages \cite{GitLab_Pages}.
The large benefit here is avoiding the cost and time of maintaining the infrastructure required to host the project's website.

It is also valuable to have developer documentation.
This documentation should contain information on the standards used in the code and how to set up a development environment, appropriate tutorials and relevant reference material.

\subsection{Support and Maintenance}
\label{Support and Maintenance}

A key activity for any piece of software is user support.
Unlike documentation this is an interactive link between developers and users.
As a minimum email support should be provided and advertised to users.
This will allow users a constructive mechanism to feedback to the development team.
The emails should be checked regularly and initial responses should be within a reasonable timeframe.
The initial response may be to ask for more details, to arrange a meeting or to say that it is being worked on.
By providing a response it signals to the user that the email is monitored and that feedback is taken seriously. 
Providing effective user support will give users more confidence in the software.

Face-to-face communication is often the most efficient and effective way to understand a user's question or comment.
This allows for a conversation to develop with probing questions improving the developer's understanding, while overcoming terminology unfamiliar to either party.
As a result it is important to put local support in place, where developers are preferably in close proximity to users.
Face-to-face contact also has the benefit of enhancing the user experience and users are more likely to report problems/bugs to the development team if they can easily hold direct discussions with them.

Forums have been used to create user communities.
Creating a user community can bring significant benefits to the software project.
The users can help solve each others problems, removing some of the burden from developers.
The other advantage is that users can see previous posts, which may have already solved their problem.
This reduces the chances of the same questions being asked to the development team on multiple occasions.

User support is about providing a constructive mechanism for feedback to the development team.
There are a few methods available to choose from; email, face-to-face and forums.
Each has their own advantages and every project will have to decide on appropriate levels of user support.
This could be a single method or a combination.
An important part of user support is to make them feel valued and listened to.

\subsection{Refactoring vs Technical Debt}
\label{Refactoring}

\todo{TODO}

\section{Packaging and Distribution}
\label{Packaging and Distribution}

\subsection{Releases}
\label{Releases}

\begin{figure}
    \centering
    \includegraphics[height=10cm]{release_workflow.eps}
    \caption{This flowchart describes the release workflow for a piece of software.}
    \label{Release_Flowchart}
\end{figure}

A good release process depends greatly on the nature of the software project.
\todo{examples from non-scientific software}

A release candidate is often used for user testing and for identifying potential issues prior to the main release.
There can be any number of release candidates and they should be generated in exactly the same manner as a release (the only difference being the version number).
Release candidates are intended for "power users" who are happy to test the software for the benefit of the wider community (for example, a scientist who is responsible for an instrument and its user programme, and who wants to ensure the software works well before providing it to their users).

The length of the beta period is determined by the release cycle and the estimated number of users.
There is no generic rule for this, but it is recommended that all (or as close to as possible) use cases of the software have been tested by specialist users.

To ensure users approach the new version with realistic expectations it is essential to provide sufficiently detailed release notes with every version of the software (including release, beta and release candidates).
These should summarise the changes made to the software since the last release, highlighting new features to incentives users to upgrade.

It is essential that all changes that may break existing workflows are listed.
This will reduce the risk of users attempting to use workflows that are no longer compatible with the latest version.

Any fixes for safety or security reasons must be clearly documented in the release notes.

It is recommended to tag and sign the release in the SCM system.
In the case of Git this involves creating a tag from the code that was used to generate the release package and signing the tag with either a project or maintainers PGP (Pretty Good Privacy) key.
This is not an essential step, but is good practice to allow verification of historic releases.

It is also recommended to generate a Digital Object Identifier (DOI) \todo{cite} for each release.
This allows the software to be easily cited in publications and makes it clear which version of the software was used to generate the results in a publication.

\todo{tidy me plz}
Prior to a release there will be a "code freeze", where no new functionality will be submitted to the code base.
In preparation for a release developers should test the software works as expected with no obvious bugs.
Once all of the identified bugs are fixed it is then released with more confidence that it is ready for general use.
It is advisable to then spend some time working on maintenance tasks, these are improvements to the maintainability of the code rather than any new user facing functionality.
Examples of such tasks include: moving to a new compiler version, applying fixes for compiler or linter warnings and code reorganisation/refactoring.

\subsection{Packaging}
\label{Packaging}

To ensure user satisfaction easy software setup is essential, but exactly what this involves varies depending on the nature of the project.
As such, this section will not go into detail about specific options, but rather set out the requirements of an installer and process of choosing an appropriate installation method for your software.

The first requirement is for the package to give users a working copy of your software.
They should not have to care about any implementation specifics or having to install additional dependencies to make your software run.
Once installed, the software should not conflict with any other software a user may have already installed on their machine.

It is recommended to sign binaries, this allows a user to verify that the copy of the software they have is in fact the version they believe it is and that it has not been modified.
Some operating systems may also impose restrictions on running unsigned binaries.
How binary signing is done depends on the packaging method and target operating system, it is worth researching this for your specific case.

As container technology becomes more popular it is worth considering as a means of distribution (for example Singularity \cite{Kurtzer_2017}).
This allows the distribution of software in an already configured and working state, which is more likely to function in a reproducible way on the user's computer irrespective of configuration or operating system.
Different containerization systems have their own caveats and there is no "one size fits all" solution.

\subsection{Cross Platform Compatibility}
\label{Cross platform}

No assumption can realistically be made on the configuration, operating system or architecture under which the software will need to run.
For this reason it is highly desirable (or even essential) that your software is designed to run on multiple operating systems and architectures.
Thanks to modern development tools, libraries and build systems this is not a complex task.
By using cross platform libraries you also allow your software to be easily adopted by users that use an operating system or environment different to your own.

If officially supporting multiple platforms then it is beneficial to encourage a spread of different platforms and operating systems across the development team.
Doing so ensures that there are always multiple people capable of addressing platform specific issues when they arise.

It is recommended to limit the number of versions of particular operating systems you support.
Typically you should stop supporting an OS version when the publisher stops supporting it.

\subsection{Dependency Management}
\label{Dependency Management}

Dependency management handles obtaining and configuring any library dependencies your software has at build/configure time.
This serves two functions: firstly, to make the build environment easy to set up and secondly, to make builds reproducible.
Dependency managers do this by collecting requirements for your software based on a specification file provided by the developer.
In the case where a package manager is available (for example {\it Conda} \cite{Conda}, {\it PyPI} \cite{PyPI} or {\it Conan} \cite{Conan}) this may just be a package name and version number.
For more general purpose tools it may involve instructions on how the dependency should be retrieved and configured (for example {\it CMake's} ExternalProject \cite{CMake_ExternalProject}).

One of the benefits of dependency management is the ability to pin dependency versions to known working versions.
This ensures that moving to new development or runtime environments will not cause incompatibilities between your software and software it depends on.
It is however important to keep dependencies up to date to ensure known bugs are not included in your software.
The task of updating dependency versions and fixing any issues that arise is a good candidate for a maintenance task (see section \ref{PM big}).

\subsection{Deployment and Infrastructure}
\label{Infrastructure}

Cloud computing is becoming an increasingly common method for delivering software to users.
Instead of the user providing the computing hardware, an interface layer (e.g. web interface or command line) is provided to a dedicated computing infrastructure.
One of the main advantages of cloud infrastructure is that the specifications of the operating system and hardware are known by the developer.
This makes it easier to recreate and fix platform specific bugs and if cloud infrastructure is used exclusively it could remove the need for cross platform development.
It is also possible to create an environment where the data is easily accessible to the software.
This removes the need for data transfer and avoids storage bottlenecks, both of which likely to become an increasing issue as data volumes grow.
However, a disadvantage of cloud infrastructure is that it creates an additional dependency.
If the cloud infrastructure is not maintained correctly then the software will become unusable within the cloud service.
The European Open Science Cloud \cite{EOSC} is one example of a project dedicated to this software delivery method.

\subsection{Licensing}
\label{Licensing}

All software requires a license.
An OSI approved \cite{OSI_Licenses} open source license is advisable from both a user and development perspective allowing users to download and use the software free of charge, removing a potential barrier in the uptake of the software.
Furthermore, in research/academic communities open source software is becoming increasingly compulsory in order to prevent the loss of valuable source code.
The European Commission's push for open science \cite{EOSC} includes both FAIR data and open source software.

\section{Project Governance}
\label{Project Governance}

Large projects with many people involved will require some form of management, to ensure that the team are working effectively.
This is often referred to as a governance model.
A governance model will provide a constructive mechanism for deciding the direction of the project and for ensuring it progresses against objectives.
This is typically achieved by using a form of project management (e.g. the APM framework \todo{cite PFQ https://www.apm.org.uk/book-shop/apm-project-fundamentals-qualification-study-guide/}), which defines a set of guidelines that will help the project be successful.
Within the APM framework, each project will have a sponsor who is ultimately responsible for the project, but will not be involved in its running.
The sponsor may take advice from several other parties.

The larger the team the more important good project management becomes.
Sections \ref{rman}, \ref{se} and \ref{lc} will discuss the aspects of project management that would benefit projects of any size.
Then section \ref{PM big} will contain advice for larger teams, but may still be useful to smaller teams.

\subsection{Resource Management}
\label{rman}

When selecting a resource for a given task it is important to consider their skills/knowledge, the time required to complete the task and the deadline for the task.
If there is a tight deadline then it is better to choose a resource who has the relevant skills and experience.
However, the draw back of always choosing the same resource to do similar tasks is that they will become a single point of failure.
If that resource were to leave the team then no-one else would have any knowledge of that part of the project.
This is a common issue in academia, due to the relatively short employment of PhD students and post-docs.
Therefore, it is advisable to use tasks as an opportunity to train less experienced/skilled resources when there is sufficient time between the deadline and the expected duration of the task.

\subsection{Stakeholder Engagement}
\label{se}

Stakeholders is a general term used to describe people who have some interest in your project (e.g. customers, team members and sponsor).
It is up to the individual project to decide which communication methods are suitable.
One method is to have an advisory committee (in the Mantid project it is called the scientific steering committee).
This group will be comprised of the senior project team (i.e. the project manager and any sub-team leaders for a larger project) and a customer from each of the main user communities.
This committee would discuss the progression of tasks and what tasks should be next (priorities).
The level of stakeholder engagement will depend on the development methodology for the project, see Section \ref{Development methodology}.
\todo{TSC}

\subsection{Project Life Cycle}
\label{lc}

The software should follow the basic notion of a release cycle. More details about the practicalities of a release cycle are discussed in section \ref{Releases}.

A release is a version of the software that has been tested and the development team is confident that it is ready to be used.
Having periodic releases can reassure users that the project is still supported.

\todo{find me a home}
It is advisable to follow semantic versioning \cite{Semantic_Versioning} (major.minor.patch, e.g. 3.1.0) for clear naming of the releases.

\subsection{Project Management Board and Advisory Committees}
\label{PM big}

It is often beneficial to set up a project management board, who discuss the high level strategy and running of the project.
These will meet fairly irregularly (a few times a year), but would involve the senior team members of the project and key stakeholders (including the sponsor).
This provides an opportunity to discuss possible risks or opportunities and the long term plans for the project.

For larger projects it maybe beneficial to have a technical advisory committee.
This will usually comprise of the technical leads for the project.
Their role is to make decisions on the computational aspects of the project, such as which version of a programming language to use and the operating systems that the software will support.
Another role fulfilled by this committee is to decide significant changes to the code base.
This could happen if there is a new method (e.g. standard library algorithms in C++) that can be used throughout the code base to improve performance and/or stability.
A representative from the technical advisory committee will report their decisions to the project management board, who will distribute the information down to the rest of the team.

\todo{mention scientific coordination?}

\section{Guidelines for development of data treatment software for neutron and muon communities}
\label{SINE2020}

This section describes guidelines specific to the development of data treatment software for the neutron and muon communities.
The specific guidelines in this section may not be suitable for other communities.

For Work Package 10 (WP10) of SINE2020, a report was written on the standards and guidelines for the participating software \cite{sine2020_wp10_d10_report}.
The software that participated in WP10 of SINE2020 are {\it BornAgain} \cite{bornagain}, {\it Mantid} \cite{mantid}, {\it McStas} \cite{mcstas}, {\it ImagingSuite} \cite{ImagingSuite}, and {\it SasView} \cite{sasview}.
The report contains the questionnaires, statistics and information collected of the software and includes a summary of the commonalities between the participating projects.
The list below presents a summary of the main guidelines from the report.

\begin{itemize}
      \item Use an Open Source license approved by the Open Source Initiative, such as GPL.
      \item Support multiple operating systems.
      \item Design software to be modular.
      \item Use the programming languages {\it C++} and/or {\it Python}.
      \item Where the software uses a GUI, use {\it Qt}.
      \item Support community standards for data formats where such exists, specifically loading of {\it NeXus}/{\it HDF5} data formats where relevant.
      \item Use {\it Git} for source code management, including user and developer documentation.
      \item Use a issue/ticketing system, such as {\it GitHub} issues.
      \item Use a tool for continuous integration and automated testing, such as {\it Travis-CI} and {\it Jenkins}.
      \item Follow a coding standard and use tools that enforces this, such as {\it Pylint} ({\it Python}) and {\it clang-format} ({\it C++}).
\end{itemize}

These guidelines contain specific recommendations for the technologies that should be used (e.g. {\it C++} and {\it Python} for programming languages), based on what is currently available.
Hence, these guidelines should be reviewed and updated regularly to ensure that the best available technological solutions are used.
Sharing the technology allows code to be developed that can easily be shared amongst collaborating projects.

The guidelines that have been developed for WP10 of SINE2020 have been used by a wide variety of software projects, see Table \ref{summaryTable} for more details.
However, the guidelines were developed from an analysis of the participating projects and significant commonalities were identified.
This was despite the variations in the size, scope and applications of the participating projects.

\begin{sidewaystable}
 \begin{tabular}{llllll}
 Software                               & {\it BornAgain} \cite{bornagain}  & {\it Mantid} \cite{mantid} &{\it McStas} \cite{mcstas}  & {\it ImagingSuite} \cite{ImagingSuite} &{\it SasView} \cite{sasview} \\
 Application                            & Multilayer scattering calculation & Data reduction and some analysis & Instrument simulations & Neutron imaging & Analysis of SANS data \\
 Age of project (years)                 & 9 & 11 & 21 & 10 & 15 \\
 Number of full time developers in 2018 & 3 & 20 & 1 & $\le$2 & $\le$2 \\
 Current number of contributors         &  & 85  & 9 & 6 & 16 \\
 Total number of contributors           &  & 194 & 47 & 11 & 69 \\
 Developer locations                    & 1 & >4 & >3 & 1 & 9 \\
 License                                & GPL3 & GPL3 & GPL2 & GPL3 & BSD \\
 Governance model                       & None & PMB \& AC  & None & AC & PMB \\
 Release cycles (per year)              & a few & 3 & 1 & 1 & 1\\
 User documentation                     & Tutorials. Examples. & Manual. Tutorials. & Manual. & Manual. Video tutorial & Manual. Tutorials. \\
 Hosting                                & {\it GitHub} & {\it GitHub} & {\it GitHub} & {\it GitHub} & {\it GitHub} \\
 Issue tracker                          & {\it Redmine} & {\it GitHub} & {\it GitHub} & {\it GitHub} & {\it JIRA} \& {\it GitHub} \\
 Python interface                       & Yes & Yes & U.D. & U.D. & Yes \\
 Primary programming language           & {\it C++} & {\it Python} / {\it C++} & {\it C} & {\it C++} & {\it Python} \\
 GUI framework                          & {\it Qt} & {\it Qt} & None & {\it Qt} & {\it Qt} \\
 Plotting library                       & {\it Matplotlib} & {\it Matplotlib} & ?? & {\it QtChart} & {\it Matplotlib} \\
 \end{tabular}
 \caption{Key parameters and analysis for the different software projects participating at the end of WP10 SINE2020.
PMB = Project Management Board.
AC = Advisory Committee.
U.D. = Under Development.}
\label{summaryTable}
\end{sidewaystable}

Some of the projects have a large number of developers and/or are based at multiple locations.
As a result the management overheads (e.g. project management boards) are greater to ensure a uniform standard across the project.
In contrast projects with fewer developers tend to have less management overhead, as it is easier to make decisions across the whole team.
This is a good example of how guidelines should be adapted to match the needs of the project.

Table \ref{summaryTable} contains some details about how plotting is used in the projects, which was not considered in the above guidelines.
It is vital for data treatment software to provide a visual representation of the data to the user.
Hence, it is worth adding a recommended plotting module to the guidelines.
As can be seen from Table \ref{summaryTable} {\it Matplotlib} is a common choice.
This is in part due to a large active developer community, which facilitates its long term sustainability.
Matplotlib also has good integration with {\it Qt}, {\it Python} prompt and {\it Jupyter notebook} making it a natural choice given previous recommendations.

\section{Conclusion}
\label{Conclusion}

Scientific software is becoming an increasingly valuable tool for research.
However, the cost of implementing such software can be high and as a result it is beneficial to write sustainable software.
The best way to achieve sustainable software is to implement guidelines.
This paper has outlined a series of guidelines for scientific software development.
The guidelines have covered a range of topics from coding standards to management of the software and customers.

Fundamentally guidelines allow for scientific software to be developed and maintained easily by a distributed team, while providing support to users.
There is an initial cost in deciding the guidelines and they should be chosen based on the requirements of the project.
To ensure that there is a balance between new functionality and maintenance it is important to manage the customer's expectations.
To ensure a happy user base it is important to have up to date documentation and to have regular releases.
The technologies available to software development are constantly changing and for long term projects it may be necessary to update the guidelines to utilise these improvements.
Good guidelines ensure a uniform code quality, a structured developer team and create a positive user experience.
These factors will help create a stable, maintainable piece of software and increase the likelihood of the software's user base flourishing.

\section{Acknowledgements}
\label{Acknowledgements}

Thanks to Ed Beard for initial proofreading.

Thanks to Robert Applin and Dimitar Tasev for additional proofreading.

Thanks to the participants in the SINE2020 WP10 Workshop II in Grenoble for valuable input and discussions.

Special thanks to all the developers who in the course of SINE2020 have contributed to the software considered in SINE2020 including converting the software to better obey the guidelines presented here.
In particularly Piotr Rozyczko, Wojciech Potrzebowski, Chiara Carminata, FZJ?, Emmanouela Rantziou, FZJ?, ILL?

This work was funded by the Horizon 2020 Framework Programme of the European Union.
Project number 654000.

%%%%%%%%%%% The bibliography starts:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  ios1.bst will be used to                               %%
%%  create a .BBL file for submission.                     %%
%%                                                         %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\nocite{*}
\bibliographystyle{ios1}
\bibliography{bibliography}

\end{document}
